{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e415a9b0-0240-4333-bc55-a330926c7246",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "### Course Machine Learning for Natural Language Understanding\n",
    "#### Instructors: Prof. Achim Rettinger, M.A. Kai Kugler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6375833-4c62-4361-a38f-6bf1ed0c78bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 18:04:20.047365: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-25 18:04:20.199655: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-25 18:04:20.199713: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-25 18:04:20.824338: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-25 18:04:20.824491: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-25 18:04:20.824512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-25 18:04:21.529281: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-25 18:04:21.529338: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-25 18:04:21.529357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-4N17KL3): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b257846c-a3ab-488e-a60d-8499d6b738ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load spacy language model for lemmatization\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# code from class exercise notebook 08\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bffb57-6209-4a10-9185-0bce41673416",
   "metadata": {},
   "source": [
    "### --- Read training data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800131db-f165-4c37-8e2f-8ac2deeb7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file1, file2):\n",
    "    with open(file1) as f:\n",
    "        clickbait_yes = [line.rstrip() for line in f]\n",
    "        df_yes = pd.DataFrame({\"headline\": clickbait_yes, \"clickbait\": \"yes\", \"label\": 1})\n",
    "    with open(file2) as f:\n",
    "        clickbait_no = [line.rstrip() for line in f]\n",
    "        df_no = pd.DataFrame({\"headline\": clickbait_no, \"clickbait\": \"no\", \"label\": 0})\n",
    "\n",
    "    return pd.concat([df_yes,df_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64888d-de3e-4cbf-b467-41545c3f277a",
   "metadata": {},
   "source": [
    "### --- Pre-Processing ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad3eff-dfd0-40d1-b966-bb372e20b982",
   "metadata": {},
   "source": [
    "##### Text Cleaning: lowercase, remove tags, remove punctuation, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d20710-8fb7-4cb7-ae98-e063877f3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    df[\"headline\"] = df[\"headline\"].str.lower()\n",
    "    df[\"headline\"] =  df[\"headline\"].str.replace(r'<[^>]*>','',regex=True)\n",
    "    df[\"headline\"] =  df[\"headline\"].str.replace(r'[^a-zA-Z ]','',regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b0446-11e7-440b-9179-a386d5b8c6ca",
   "metadata": {},
   "source": [
    "\n",
    "##### Tokenization, stopword removal, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea63dce6-fae5-4f7b-8508-db43b2eb42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    lemmatized_sent = [word.lemma_ for word in nlp(\" \".join(tokenized_sent))]\n",
    "    return lemmatized_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbf933-fc77-4eeb-9a67-b7e012a8dcdd",
   "metadata": {},
   "source": [
    "### ---TF-IDF Vectorizer---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9524a556-2df6-47e4-af04-619f761908e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train, test):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenize, token_pattern=None)\n",
    "    vectorizer.fit(train)\n",
    "    \n",
    "    train = vectorizer.transform(train)\n",
    "    test = vectorizer.transform(test)\n",
    "    \n",
    "    pickle.dump(vectorizer, open(\"tfidf.pkl\", \"wb\"))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42aa57-d59a-4888-af49-a016a0f00c32",
   "metadata": {},
   "source": [
    "### ---Classifier---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6132f17-b3b0-4e96-8333-691dee7e6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_model(X, y):\n",
    "    model=MultinomialNB(alpha=0.5) \n",
    "    model.fit(X,y)\n",
    "    pickle.dump(model, open('NB_model.pkl', 'wb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7333896-f685-4ea9-91a2-c35a95ef0603",
   "metadata": {},
   "source": [
    "### ---Evaluation---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea57a5c-2c1a-4852-90c0-2769fa6afb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y_test):\n",
    "    return precision_recall_fscore_support(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfe179-1c60-4838-a26f-d6b414b3e2ef",
   "metadata": {},
   "source": [
    "# Build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f4a662-9bb6-4403-a59a-9af11e88f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(file1, file2):\n",
    "    df = read_data('clickbait_yes', 'clickbait_no')\n",
    "    \n",
    "    df = clean(df)\n",
    "    \n",
    "    y = df['label']  # label 0 = no, 1 =yes clickbait\n",
    "    X = df[\"headline\"]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.15, shuffle=True)\n",
    "    \n",
    "    X_train, X_test = vectorize(X_train, X_test)\n",
    "    \n",
    "    bayes = NB_model(X_train,y_train)\n",
    "    \n",
    "    y_pred = bayes.predict(X_test)\n",
    "    validation_evaluation = evaluate(y_pred, y_test)\n",
    "    return validation_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5c745-0cfd-4c1a-adbb-594f84b95103",
   "metadata": {},
   "source": [
    "### ---Use test data with model---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82d0d96d-f18b-49cb-8b0e-cd42a20e2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_out_data(file):\n",
    "    df = pd.read_csv(file, sep=\";\")\n",
    "    df = clean(df)\n",
    "    X = df[\"headline\"]\n",
    "    \n",
    "    #load vectorizer\n",
    "    vectorizer = pickle.load(open('tfidf.pkl', 'rb'))\n",
    "    X = vectorizer.transform(X)\n",
    "    \n",
    "    #load model\n",
    "    model = pickle.load(open('NB_model.pkl', 'rb'))\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccf1c7-b2e3-46c0-82e5-18db639af098",
   "metadata": {},
   "source": [
    "## Validation Set Evaluation result (F1, precision, recall, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe14f899-78e8-4dac-a03f-cc8a51176ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9691862845840641, 0.9686187544733202, 0.9687335797011452, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_classifier(\"clickbait_no.txt\", \"clickbait_yes.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c72c8-7c6a-4ba5-a70c-76ca738782a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d8da773-a331-4f7d-9d34-930a13bcef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classify_test_out_data(\"clickbait_hold_X.csv\")\n",
    "with open(\"predictions.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(str(pred) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1270c1-91aa-4505-a5ba-2df55c8ffaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
